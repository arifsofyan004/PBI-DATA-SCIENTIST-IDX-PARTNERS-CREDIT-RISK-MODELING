# -*- coding: utf-8 -*-
"""Data Scientist Intern_IDX_Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KjYPKl7pGxeplwg9LkrAMT8xzk30y2e2

- Nama: Mohamad Arif Sofyan
- PBI : ID/X Parnets

# Import Data
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/Dataset/loan_data_2007_2014.csv")
df.head()

"""# Data Understanding"""

df.shape

df.info()

# Mengatur opsi tampilan Pandas
pd.set_option('display.max_rows', None)  # Memastikan semua baris dapat ditampilkan
pd.set_option('display.max_columns', None)  # Memastikan semua kolom dapat ditampilkan
pd.set_option('display.width', 1000)  # Mengatur lebar tampilan agar cukup untuk menampilkan semua data

# Menampilkan jumlah nilai yang hilang untuk setiap kolom
print("Jumlah Nilai yang Hilang per Kolom:")
print(df.isnull().sum())

"""# Pemilihan feature

# Keterangan untuk Kolom yang Dihapus:

**Kolom Redundan atau Tidak Relevan:**
- 'Unnamed: 0': **hanya indeks tambahan**.
- 'id', 'member_id', 'application_type': **ID unik yang tidak mempengaruhi analisis.**
- 'url': **URL dari pinjaman, tidak relevan untuk analisis.**
- 'desc': **Deskripsi teks bebas, sulit diinterpretasikan untuk model.**
- 'policy_code': **Jika semua nilai sama, tidak memberikan informasi.**
- 'title' : **Terlalu banyak informasi dan sebagai gantinya ada kolom purpose**

**Kolom dengan Banyak Data yang Hilang:**
- 'annual_inc_joint', 'dti_joint', 'verification_status_joint': **Mayoritas data tidak tersedia.**
- 'open_acc_6m', 'open_il_6m', dll.: **Jumlah data yang hilang signifikan.**
- 'mths_since_rcnt_il', 'total_bal_il', dll.: **Informasi kurang lengkap mengenai kredit.**
- 'mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog', 'next_pymnt_d': **Tingkat kekosongan data sangat tinggi.**

**Kolom Terlalu Spesifik:**
- 'zip_code', 'addr_state': **Informasi geografis yang terlalu spesifik dan kurang relevan untuk analisis umum.**
"""

columns_to_drop = [
    'Unnamed: 0', 'id', 'member_id', 'url', 'desc', 'policy_code',
    'annual_inc_joint', 'dti_joint', 'verification_status_joint',
    'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',
    'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',
    'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',
    'inq_last_12m', 'mths_since_last_delinq', 'mths_since_last_record',
    'mths_since_last_major_derog', 'next_pymnt_d', 'zip_code', 'addr_state','application_type','title'
]

df = df.drop(columns=columns_to_drop)
df.head()

df.info()

"""# CLEANING, PREPROCESSING, FEATURE ENGINEERING
Pada tahap ini, dilakukan penghapusan atau modifikasi beberapa fitur agar dapat disusun dalam format yang cocok untuk proses pemodelan.

# emp_length
Memodifikasi `emp_length`. Contoh: 8 years -> 8
"""

df['emp_length'].unique()

df['emp_length_int'] = df['emp_length'].str.replace('\+ years', '', regex=True)
df['emp_length_int'] = df['emp_length_int'].str.replace('< 1 year', str(0), regex=True)
df['emp_length_int'] = df['emp_length_int'].str.replace(' years', '', regex=True)
df['emp_length_int'] = df['emp_length_int'].str.replace(' year', '', regex=True)

# Ubah tipe data kolom emp_length_int menjadi float
df['emp_length_int'] = df['emp_length_int'].astype(float)

# Hapus kolom emp_length yang sudah tidak diperlukan
df.drop('emp_length', axis=1, inplace=True)
df['emp_length_int'].head()

"""# term
Memodifikasi `term`. Contoh: 60 months -> 60
"""

df['term'].unique()

df['term_int'] = df['term'].str.replace(' months', '').astype(int)

df.drop('term', axis=1, inplace=True)

"""# earliest_cr_line
Mengubah format `earliest_cr_line` dari bulan-tahun menjadi perhitungan jumlah waktu yang telah berlalu sejak saat itu. Untuk melakukan ini, biasanya digunakan tanggal referensi = hari ini. Tetapi, karena dataset ini berkisar antara tahun 2007-2014, lebih relevan untuk menggunakan tanggal referensi sekitar tahun 2017. Dalam kasus ini, tanggal 2017-12-01 digunakan sebagai tanggal referensi.
"""

df.earliest_cr_line.head()

df['earliest_cr_line_date'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['earliest_cr_line_date'].head()

import numpy as np

df['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
df['mths_since_earliest_cr_line'].head(3)

df['mths_since_earliest_cr_line'].describe()

"""Terlihat ada nilai yang aneh, yaitu negatif."""

latest_year = df['earliest_cr_line_date'].dt.year.max()

print("Tahun terbaru dalam kolom 'earliest_cr_line' adalah:", latest_year)

"""Nilai negatif muncul karena fungsi Python salah menafsirkan tahun 68 sebagai tahun 2068, padahal seharusnya 68 merujuk pada tahun 1968."""

df[df['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head(3)

"""Untuk menyelesaikan masalah ini, kita bisa melakukan preprocessing lebih lanjut untuk memperbaiki tahun 2068 menjadi 1968. Namun, pada kali ini, saya hanya akan mengubah nilai yang negatif menjadi nilai maksimum dari fitur tersebut. Dalam kasus ini, karena nilai negatif menunjukkan data yang sangat lama (tahun 1900-an), masih masuk akal jika saya menggantinya dengan nilai maksimum."""

df.loc[df['mths_since_earliest_cr_line']<0, 'mths_since_earliest_cr_line'] = df['mths_since_earliest_cr_line'].max()

df.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""# issue_d
Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

df['issue_d_date'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df['issue_d_date']) / np.timedelta64(1, 'M')))

df['mths_since_issue_d'].describe()

df.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""# last_pymnt_d
Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

df['last_pymnt_d_date'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%y')
df['mths_since_last_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df['last_pymnt_d_date']) / np.timedelta64(1, 'M')))

df['mths_since_last_pymnt_d'].describe()

df.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""### last_credit_pull_d
Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

df['last_credit_pull_d_date'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%y')
df['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df['last_credit_pull_d_date']) / np.timedelta64(1, 'M')))

df['mths_since_last_credit_pull_d'].describe()

df.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

"""# **DEFINE TARGET VARIABLE / LABELING**

Menentukan target variabel good loan atau bad loan dari informasi kolom **loan_status**

Dalam proyek pemodelan risiko kredit, tujuan utamanya adalah untuk memprediksi kemampuan seseorang dalam melakukan pembayaran pinjaman atau kredit yang diberikan. Oleh karena itu, variabel target yang dipilih harus mencerminkan kemampuan individu

variabel `loan_status` cocok digunakan sebagai variabel target karena mencerminkan seberapa baik individu tersebut dalam melakukan pembayaran terhadap pinjaman atau kredit mereka selama ini.
"""

df.loan_status.value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

plt.figure(figsize=(10, 6))
sns.countplot(y='loan_status', data=df)
plt.title('Distribusi Loan Status')
plt.show()

"""- **Current**: Pinjaman yang sedang aktif dan pembayarannya dilakukan tepat waktu.

- **Fully Paid**: Pinjaman ini telah dilunasi sepenuhnya, tanpa keterlambatan pembayaran.

- **Charged Off**: Pinjaman ini telah dianggap sebagai kerugian oleh pemberi pinjaman, biasanya setelah periode keterlambatan pembayaran yang signifikan.

- **Late (31-120 days)**: Pinjaman ini terlambat dibayar antara 31 hingga 120 hari.

- **In Grace Period**: Pinjaman ini dalam periode tenggang, yaitu periode waktu singkat setelah jatuh tempo pembayaran di mana peminjam masih dapat melakukan pembayaran tanpa dikenakan denda.

- **Does not meet the credit policy. Status: Fully Paid**: Pinjaman ini telah dilunasi sepenuhnya tetapi tidak memenuhi kebijakan kredit yang ditetapkan setelah pinjaman diberikan.

- **Late (16-30 days)**: Pinjaman ini terlambat dibayar antara 16 hingga 30 hari.

- **Default**: Pembayaran macet, biasanya ini berarti pembayaran telah terlambat lebih dari 120 hari.

- **Does not meet the credit policy. Status: Charged Off**: Pinjaman ini telah dianggap sebagai kerugian dan tidak memenuhi kebijakan kredit yang ditetapkan setelah pinjaman diberikan.

Membuat feature target untuk mengklasifikasikan status pinjaman menjadi Good Loan atau Bad Loan. Good Loan dikategorikan sebagai 1 dan Bad Loan sebagai 0.
"""

def classify_loan(status):

    # Mendefinisikan status pinjaman yang dikategorikan sebagai Bad Loan
    bad_loan_statuses = [
        'Charged Off',
        'Default',
        'Does not meet the credit policy. Status:Charged Off',
        'Late (31-120 days)',
        'Late (16-30 days)'
    ]

    # Mengembalikan 0 jika status termasuk dalam bad_loan_statuses, selain itu 1
    return 0 if status in bad_loan_statuses else 1

# Menerapkan fungsi classify_loan pada DataFrame untuk membuat kolom target baru
df['loan_status_target'] = df['loan_status'].apply(classify_loan)

# Menampilkan beberapa baris pertama dari DataFrame untuk memverifikasi hasil
print(df[['loan_status', 'loan_status_target']].head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.countplot(x='loan_status_target', data=df)
plt.title('Distribusi Good Loan vs Bad Loan')
plt.xlabel('Loan Status (0 = Bad Loan, 1 = Good Loan)')
plt.ylabel('Jumlah')
plt.show()

"""dapat dilihat bahwa jumlah bad loan jauh lebih sedikit daripada good loan. Hal ini menyebabkan problem imbalanced dataset.

Jangan lupa untuk membuang kolom asal `loan_status`
"""

df.drop('loan_status', axis=1, inplace=True)

"""# Exploratory Data Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(numeric_only=True))

"""Di sini, jika terdapat pasangan fitur dengan korelasi yang tinggi, salah satunya akan dipilih. Tidak ada standar pasti untuk menentukan nilai korelasi yang dianggap tinggi, tetapi biasanya angka 0.7 digunakan sebagai acuan."""

# Hitung matriks korelasi antar fitur numerik
corr_matrix = df.corr(numeric_only=True).abs()

# Ambil setengah bagian atas dari matriks korelasi untuk mendapatkan nilai korelasi yang unik
# Gunakan np.triu untuk menghasilkan matriks segitiga atas dengan nilai True di atas diagonal
# Dengan parameter k=1 untuk menghilangkan diagonal utama
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Temukan kolom-kolom yang memiliki korelasi lebih dari 0.7 dengan kolom lainnya
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

to_drop_hicorr

df.drop(to_drop_hicorr, axis=1, inplace=True)

"""# Check Categorical Features"""

df.select_dtypes(include='object').nunique()

"""Pada tahap ini dilakukan pembuangan fitur yang memiliki nilai unik yang sangat tinggi (high cardinality)"""

df.drop(['emp_title'], axis=1, inplace=True)

for col in df.select_dtypes(include='object').columns.tolist():
    print(df[col].value_counts(normalize=True)*100)
    print('\n')

"""Fitur yang sangat didominasi oleh salah satu nilai saja akan dibuang pada tahap ini."""

df.drop('pymnt_plan', axis=1, inplace=True)

"""# MISSING VALUES
Missing Value Checking
"""

check_missing = df.isnull().sum() * 100 / df.shape[0]
check_missing[check_missing > 0].sort_values(ascending=False)

"""Proses Impute missing value setelah dataset splitting agar tidak terjadi data leakage

# Dataset Splitting
"""

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

X = df.drop(['loan_status_target'], axis=1)
y = df['loan_status_target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# Preprocessor"""

df.info()

# Tentukan kolom numerik dan kategorikal
numerical_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths',
                  'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp',
                  'total_rec_late_fee', 'recoveries', 'collections_12_mths_ex_med', 'acc_now_delinq',
                  'tot_coll_amt', 'tot_cur_bal', 'emp_length_int', 'term_int', 'mths_since_earliest_cr_line',
                  'mths_since_issue_d']

categorical_cols = ['grade', 'sub_grade', 'home_ownership', 'verification_status',
                    'purpose', 'initial_list_status']

# Buat pipeline untuk preprocessing
numerical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler()),
])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ("numeric", numerical_pipeline, numerical_cols),
    ("categoric", categorical_pipeline, categorical_cols)
])

"""Impute missing value untuk numerik menggunakan mean dan kategorik dengan modus. Serta dilakukan Scalling untuk data numerik menggunakan standardScaler dan dilakukan encoding untuk data kategorik menggunakan OneHotencoder

# Modeling
"""

rf_model = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(random_state=42))
])

# Menambahkan parameter rf
param_space = {
    "classifier__n_estimators": [100, 200, 300],
    "classifier__max_depth": [5, 10, 15],
    "classifier__min_samples_split": [2, 5, 10],
    "classifier__min_samples_leaf": [1, 2, 4]
}

# Inisialisasi pencarian acak dengan nama model
model = RandomizedSearchCV(rf_model, param_distributions=param_space, n_iter=10, cv=3, random_state=42)

model.fit(X_train, y_train)

# Tampilkan hasil
print("Best Parameters:", model.best_params_)
print("Training Accuracy:", model.score(X_train, y_train))
print("Model Best Score:", model.best_score_)
print("Test Accuracy:", model.score(X_test, y_test))

"""# Evaluasi model"""

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

